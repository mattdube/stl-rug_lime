<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Maching Learning in R with mlr3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Matt Dube" />
    <link href="libs/remark-css/mlr3.css" rel="stylesheet" />
    <link href="libs/remark-css/middlebury-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Maching Learning in R with mlr3
## with mlr3
### Matt Dube
### 2019/09/26 (updated: 2019-10-01)

---




# Intro and a few notes
Topic Overview
- what we won't be covering
  - this is not a machine learning how-to
  - we won't be going in-depth on feature engineering, resampling, model selection, etc.

- what we will be covering
  - overview of mlr3 and the mlr3 family of packages
  - several small demos, and
  - a bigger demo that puts it all together

- the dataset we'll be using for the demos is the attrition dataset from the rsample package.
 - I've set aside 15% of the dataset to be used as "new data"

&lt;b&gt;Keep in mind:&lt;/b&gt;
- mlr3 (and extension packages) are under active development

- examples today may not  work next week

- if something breaks, the [documention](https://mlr3.mlr-org.com/index.html) is very good, and updated often

- this is a &lt;strong&gt;BIG&lt;/strong&gt; topic, we won't cover it all
 - read the docs, and check the references at the end of this deck
 
- there is some different terminology and syntax
 - don't get hung up on the differences and quirks

- some of this may offend your normal R sensibilities

---

# Machine Learning in R: Current State
.pull-left[
.large[&lt;strong&gt;Individual Packages&lt;/strong&gt;

- C5.0
- earth
- glm
- glmnet
- keras
- randomForest
- ranger
- rpart
- xgboost
- many others!]
]

.pull-right[
.large[&lt;strong&gt;Meta Packages&lt;/strong&gt;

Current Generation
- Caret
- mlr

Next Generation
- tidymodels
- mlr3]
]

R does not define a standardized interface for its machine-learning algorithms. Therefore, for any non-trivial experiments, you need to write lengthy, tedious and error-prone wrappers to call the different algorithms and unify their respective output.

Additionally you need to implement infrastructure to:

- resample your models
- optimize hyperparameters
- select features
- cope with pre- and post-processing of data and compare models in a statistically meaningful way.


---
# Why mlr3?
&lt;strong&gt;mlr&lt;/strong&gt;

- mlr was first released to CRAN in 2013

- now has over &lt;b&gt;&lt;font color = "red"&gt;30,000&lt;/font&gt;&lt;/b&gt; lines of code

- over &lt;b&gt;&lt;font color = "red"&gt;120&lt;/font&gt;&lt;/b&gt; direct dependencies and over &lt;strong&gt;1400&lt;/strong&gt; indirect dependencies

- dependency API changes break mlr

- unit testing takes a long time, and most tests are disabled to comply with CRAN policies

- S3 object oriented (OO) toolkit has limitations in large projects

- NAMESPACE has &gt; &lt;b&gt;&lt;font color = "red"&gt;1200&lt;/font&gt;&lt;/b&gt; lines, &gt; 440 export functions and objects.

- only works on in-memory data

- no nested parallelization

- too difficult to extend, maintain, and add new features 

- complex code base makes it difficult for new contributors

---
# mlr3 architectural changes
&lt;strong&gt;mlr3&lt;/strong&gt;
- utilizes R6 OO toolkit

- truly object-oriented: data and methods together

  - inheritance
  - reference semantics

- embrace data.table, both for arguments and internal data structures

- light on dependencies 
 - ~ 10 packages – 4 of which are developed by mlr team
 - R6, data.table, Metrics, lgr, digest, uuid, mlbench 

- utilizes future and future.apply for parallelization

- only basic building blocks are in the core mlr3 package

- extra packages contain additional features and capabilities

---
# mlr3 workflow
.pull-left[
The mlr3 package provides R6 classes for the essential building blocks of the machine learning workflow:
- A &lt;strong&gt;task&lt;/strong&gt; encapsulates the data along with additional information, such as what the prediction target is.

- A &lt;strong&gt;learner&lt;/strong&gt; encapsulates one of R’s many machine learning algorithms and allows to train models and make predictions. Most learners have hyperparameters that affect their operation.

- A &lt;strong&gt;measure&lt;/strong&gt; computes a numeric score based on predicted and ground-truth values and their difference.

- A &lt;strong&gt;resampling&lt;/strong&gt; strategy specifies a series of train and test sets and is used to assess the performance of an algorithm.
]

.pull-right[
![mlr3 workflow.](media/workflow.png)
]

---

# mlr3 and its extension packages

| Package | Functionality |
| :-      | :------------ |
| `mlr3`  | Framework for machine learning: `Task`, `Learner`, `resample()` and `benchmark()` |
| `mlr3learners` | Concrete `Learner`s for many popular machine learning implementations |
| `mlr3pipelines` | Dataflow programming of machine learning workflows. |
| `mlr3tuning` | Hyperparameter tuning for machine learning algorithms. |
| `mlr3filter` | Feature filtering |
| `mlr3viz` | Visualisations and plots |
| `paradox` | Auxiliary package providing (hyper)parameter handling |
| `mlr3misc` | Auxiliary functions |
| `mlr3fswrap` | Variable selection wrappers |
| `mlr3survival` | Extensions for survival analysis |
| `mlr3ordinal` | Extensions for ordinal regression |
| `mlr3spatiotemporal` | Extensions for spatiotemporal resampling methods |
| `mlr3hyperband` | Implements the "hyperband" approach for hyperparameter tuning |

???
mlr3fswrap	Variable selection wrappers like sequential forward/backward search, exhaustive search, or genetic algorithms.
---
# mlr3 extension packages &lt;strong&gt;planned&lt;/strong&gt;

| Package | Functionality |
| :-      | :------------ |
| `mlr3cluster` | Extensions for cluster analysis |
| `mlr3fda` | Extensions for functional data analysis |
| `mlr3forecasting` | Extensions for forecasting |
| `mlr3keras` | Extnensions for deep learning via keras |
| `mlr3extralearners` | Extension to source additional learners from remote sources |

---

# mlr3 overview
pre-defined tasks, learners, resamplings, and measures are stored in the following R6 dictionaries:
- mlr_tasks
- mlr_learners
- mlr_resamplings
- mlr_measures


&lt;b&gt;mlr_tasks: built-in data sets&lt;/b&gt;

```r
as.data.table(mlr_tasks)[,.(key, task_type)]
```

```
##               key task_type
## 1: boston_housing      regr
## 2:  german_credit   classif
## 3:           iris   classif
## 4:         mtcars      regr
## 5:           pima   classif
## 6:          sonar   classif
## 7:           spam   classif
## 8:           wine   classif
## 9:            zoo   classif
```

---
# mlr3 learners: base package

```r
as.data.table(mlr_learners)[,list(key,packages, predict_types)]
```

```
##                    key packages predict_types
## 1:       classif.debug          response,prob
## 2: classif.featureless          response,prob
## 3:       classif.rpart    rpart response,prob
## 4:    regr.featureless    stats   response,se
## 5:          regr.rpart    rpart      response
```

---

# mlr3 learners: extension package loaded

```r
library(mlr3learners)
as.data.table(mlr_learners)[,list(key,packages, predict_types)]
```

```
##                     key    packages predict_types
##  1:       classif.debug             response,prob
##  2: classif.featureless             response,prob
##  3:      classif.glmnet      glmnet response,prob
##  4:        classif.kknn  withr,kknn response,prob
##  5:         classif.lda        MASS response,prob
##  6:     classif.log_reg       stats response,prob
##  7: classif.naive_bayes       e1071 response,prob
##  8:         classif.qda        MASS response,prob
##  9:      classif.ranger      ranger response,prob
## 10:       classif.rpart       rpart response,prob
## 11:         classif.svm       e1071 response,prob
## 12:     classif.xgboost     xgboost response,prob
## 13:    regr.featureless       stats   response,se
## 14:         regr.glmnet      glmnet      response
## 15:           regr.kknn  withr,kknn      response
## 16:             regr.km DiceKriging   response,se
## 17:             regr.lm       stats   response,se
## 18:         regr.ranger      ranger   response,se
## 19:          regr.rpart       rpart      response
## 20:            regr.svm       e1071      response
## 21:        regr.xgboost     xgboost      response
##                     key    packages predict_types
```

---

# mlr3 resamplings

```r
as.data.table(mlr_resamplings)
```

```
##            key                 params iters
## 1:   bootstrap stratify,repeats,ratio    30
## 2:      custom                            0
## 3:          cv         stratify,folds    10
## 4:     holdout         stratify,ratio     1
## 5: repeated_cv stratify,repeats,folds   100
## 6: subsampling stratify,repeats,ratio    30
```

---

# mlr3 measures
.pull-left[

```r
as.data.table(mlr_measures)[task_type == "classif"][,.(key)]
```

```
##                     key
##  1:         classif.acc
##  2:         classif.auc
##  3:          classif.ce
##  4:       classif.costs
##  5:         classif.dor
##  6:     classif.f_score
##  7:         classif.fdr
##  8:          classif.fn
##  9:         classif.fnr
## 10:         classif.for
## 11:          classif.fp
## 12:         classif.fpr
## 13:         classif.npv
## 14:         classif.ppv
## 15:   classif.precision
## 16:      classif.recall
## 17: classif.sensitivity
## 18: classif.specificity
## 19:          classif.tn
## 20:         classif.tnr
## 21:          classif.tp
## 22:         classif.tpr
##                     key
```
]

.pull-right[

```r
as.data.table(mlr_measures)[task_type != "classif", .(key)]
```

```
##          key
## 1:  regr.mae
## 2:  regr.mse
## 3: regr.rmse
```

]

---

# mlr3 filters
|Name |	Task Type |	Feature Types |	Package |
| :-  | :---------| :-------------| :-------|
| `anova` |	Classif | Integer, Numeric | stats |
| `auc` | Classif |	Integer, Numeric | Metrics|
| `carscore` | Regr |	Numeric | care
| `cmim` | Classif &amp; Regr |	Integer, Numeric, Factor, Ordered |	praznik |
| `correlation` | Regr |	Integer, Numeric | stats |
| `disr` | Classif |	Integer, Numeric, Factor, Ordered |	praznik |
| `importance` | Universal |	Logical, Integer, Numeric, Character, Factor, Ordered |	rpart|
| `information_gain` | Classif &amp; Regr |	Integer, Numeric, Factor, Ordered |	FSelectorRcpp|
| `jmi` | Classif |	Integer, Numeric, Factor, Ordered |	praznik |
| `jmim` | Classif |	Integer, Numeric, Factor, Ordered |	praznik |
| `kruskal_test` |	Classif |	Integer, Numeric |	stats |
| `mim` | Classif |	Integer, Numeric, Factor, Ordered |	praznik |
| `mrmr` | Classif &amp; Regr |	Numeric, Factor, Integer, Character, Logical |	praznik |
| `njmim` |	Classif	Integer, Numeric, Factor, Ordered |	praznik |
| `performance` | Universal |	Logical, Integer, Numeric, Character, Factor, Ordered |	rpart |
| `variance` | Classif &amp; Regr |	Integer, Numeric | stats |

---

# installation

Install the mlr3 packages from [Github](https://github.com/mlr-org):


```r
remotes::install_github("mlr-org/mlr3")
remotes::install_github("mlr-org/mlr3db")
remotes::install_github("mlr-org/mlr3viz")
remotes::install_github("mlr-org/mlr3learners")
remotes::install_github("mlr-org/mlr3tuning")
remotes::install_github("mlr-org/mlr3filters")
remotes::install_github("mlr-org/mlr3pipelines")
```


.footnote[
[1] Check [mlr3 github page](https://github.com/mlr-org/mlr3) for current installation instructions 

[2] See current list of [mlr3 extension packages](https://github.com/mlr-org/mlr3/wiki/Extension-Packages) on github  
]

---

# modeling
Considering how we are going to tackle the problem relates closely to what `mlr3` entities we will use.

- What is the problem we are trying to solve?
  - i.e. what **Task** do we use?
  - Binary classification.
  - `\(\Rightarrow\)` We use `TaskClassif`.
- What are appropriate learning algorithms?
  - i.e. what **Learner** do we use?
  - Logistic regression, CART, Random Forest
  - `\(\Rightarrow\)` `lrn("classif.log_reg")`, `lrn("classif.rpart")`, `lrn("classif.ranger")`
- How do we evaluate "good" performance? `\(\Rightarrow\)` Depends on many things! Cost of false positive vs. false negative, legal requirements, ...
  - i.e. what **Measure** do we use?
  - We start with misclassification error and will also consider AUC.
  - `\(\Rightarrow\)` `msr("classif.ce")`, `msr("classif.auc")`
  
---
# create task

a new classification task is created with:

`task = TaskClassif$new(id = "GermanCredit", backend = credit, target = "class")`

- id = task identifier, optional
- backend = backend sets the dataset for the task. 
- target =  target variable in the backend dataset. 
- positive = optional - which factor of the target variable is the positive class.

The created task is an R6 object. There are several methods that can be used to review the task details.

| method/field | outputs |
| :-      | :------------ |
| `task` | review task properties |
| `task$ncol` | number of columns in task (dataset)|
| `task$nrow` | number of rows in task (dataset) |
| `task$formula()` | default modeling formula |
| `task$feature_names` | list of feature names |
| `task$feature_types` | feature data types |
| `task$class_names` | target variable classes |
| `task$col_roles` | role of each column (typically features and target) |
| `task$data()` | dataset defined in task |

---

# create learners

`lrn("classif.ranger")`

`lrn("classif.xgboost")`

You can query a list of learners with 

`as.data.table(mlr_learners)`

Review learner:

```r
lrn("classif.xgboost")
```

```
## &lt;LearnerClassifXgboost:classif.xgboost&gt;
## * Model: -
## * Parameters: nrounds=1, verbose=0
## * Packages: xgboost
## * Predict Type: response
## * Feature types: integer, numeric
## * Properties: importance, missings, multiclass, twoclass, weights
```


---

# split data

In mlr3, we do not create train and test data sets, but instead keep only a vector of train and test indices.

`set.seed(4411)`

`train.idx &lt;- sample(seq_len(task$nrow), 0.7 * task$nrow)`

`test.idx &lt;- setdiff(seq_len(task$nrow), train.idx)`

---

class: inverse, middle, center

# Deck not Complete 

Need links to:
- demo markdown html files
- additional online resources

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
